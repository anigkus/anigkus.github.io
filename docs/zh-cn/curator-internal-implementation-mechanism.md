<iframe src="../detail-header.html" title="Github of Anigkus" style="height:0px,widht:0px;display:none" id="kusifreamheader"></iframe>

<h1 style="color:#606c71;text-align:center;" id="h1" >Curator å†…éƒ¨å®ç°æœºåˆ¶</h1><br/>

[<h1 style="color:#606c71;text-align:center;" >Curator internal implementation mechanism</h1><br/>]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-1.jpeg" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

> <br/>&nbsp;&nbsp;&nbsp;&nbsp; [Zookeeper](https://zookeeper.apache.org/) å¯¹å¤§å®¶éƒ½ä¸æ˜¯å¾ˆé™Œç”Ÿ,ç°åœ¨å¥½å¤šå¼€æºçš„ä¸­é—´ä»¶éƒ½åœ¨ä½¿ç”¨ Zookeeper æ¥ä½œä¸ºåˆ†å¸ƒå¼åè°ƒä¸­å¿ƒæœåŠ¡.é‚£ä¹ˆJavaä¸­æ“ä½œ Zookeeperçš„å®¢æˆ·ç«¯æœ‰ Zookeeper åŸç”Ÿæä¾›çš„ã€å¼€æº [zkclient](https://github.com/sgroschupf/zkclient) ä»¥åŠ [Apache Curator](https://curator.apache.org/). è€Œ Zookeeper åŸç”Ÿç®—æ˜¯æ¯”è¾ƒåº•å±‚,æ“ä½œèµ·æ¥ç‰¹åˆ«ä¸æ–¹ä¾¿,æ¥å£å’Œæ–¹æ³•è¡¨è¾¾çš„æ–¹å¼ä¸å¤Ÿç›´æ¥,å¹¶ä¸”è¿˜æœ‰ä¸å°‘é—®é¢˜.è€Œ zkclient æ˜¯å¯¹ Zookeeper åŸç”Ÿå°è£…äº†ä¸€å±‚,ä½†æ˜¯å…¶ä¸­çš„æ–‡æ¡£ä¸è¶³,ä»¥åŠé‡è¯•ã€å¼‚å¸¸ç­‰æœºåˆ¶æœ‰ä¸å°‘é—®é¢˜,ä¹Ÿä¸€ç›´è¢«ç¤¾åŒºæ‰€è¯Ÿç—….é‚£ä¹ˆæœ‰æ²¡æœ‰ä¸€æ¬¾ç°åœ¨æ¯”è¾ƒå¥½çš„å®¢æˆ·ç«¯å‘¢,é‚£å°±æ˜¯ Curator.è¿™ç¯‡æ–‡ç« æˆ‘å°†ä»å†…éƒ¨è§’åº¦æ¥åˆ†æä¸‹ Curator çš„å®ç°æœºåˆ¶.<br/>
> <br/>

[> <br/>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ???Zookeeper+++(https://zookeeper.apache.org/) is not very unfamiliar to everyone. Now many open source middleware are using Zookeeper as a distributed coordination center service. Then the client that operates Zookeeper in Java has Zookeeper Natively provided, open source ???zkclient+++(https://github.com/sgroschupf/zkclient) and ???pache Curator+++(https://curator.apache.org/). Zookeeper is relatively low-level, and it is very difficult to operate. Convenience, the way of expressing interfaces and methods is not direct enough, and there are still many problems. The zkclient is a native encapsulation of Zookeeper, but the documentation is insufficient, and there are many problems in the retry, exception and other mechanisms, and it has been used for a long time. The community criticized. So is there a better client now, that is Curator. In this article, I will analyze the implementation mechanism of Curator from an internal perspective.<br/>]:#
[> <br/>]:#

# ä»€ä¹ˆæ˜¯ä½¿ç”¨å¼€æºçš„æ­£ç¡®å§¿åŠ¿?

[# What's the right posture to use open source?]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-2.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

* All-takenism: ç§°ä¹‹â€œæ‹¿æ¥ä¸»ä¹‰â€é˜¶æ®µ,ä¸ç®¡å®ƒæ˜¯æ€ä¹ˆæ ·çš„,å…ˆç”¨ç€å†è¯´.

* Know the roots: åˆ°åæ¥ç”¨ç€ç”¨ç€æ—¶é—´ä¹äº†,ä¹Ÿå°±è¶Šæ¥çº¦ç†Ÿæ‚‰äº†,å¯¹äº›åŸç†å’Œé—®é¢˜ä¹Ÿäº†è§£çš„å·®ä¸å¤šäº†,ç§°ä¹‹â€œçŸ¥æ ¹çŸ¥åº•â€é˜¶æ®µ.

* Patch up: å†åˆ°åæ¥ç”¨çš„è¿‡ç¨‹ä¸­,æ€»ä¼šå‘ç°äº›é—®é¢˜æˆ–è€…æŸäº›åŠŸèƒ½ä¸å¦‚æ„.ç”±äºæˆ‘ä»¬çŸ¥é“äº†å®ƒæ€ä¹ˆå¼„çš„,å°±ä¼šå°è¯•ç€å»æ›´æ”¹å®ƒçš„é—®é¢˜æˆ–è€…å¢åŠ åŠŸèƒ½,è¿™ä¸ªé˜¶æ®µç§°ä¹‹ä¸ºâ€œä¿®ä¿®è¡¥è¡¥â€.

* Make a new start: ç­‰åˆ°å®Œå…¨åƒé€äº†,å‘ç°ç®€å•çš„ä¿®æ”¹æˆ–è€…å¢åŠ åŠŸèƒ½æ ¹æœ¬æ²¡æ³•æ”¹å˜åº•å±‚è¶Šæ¥ä¸è¶³çš„åœ°æ–¹.è¿™ä¸ªæ—¶å€™å‘ç°æŠ•å…¥çš„è¡¥ä¸æ—¶é—´æ¯”è¶Šæ¥è¶Šå¤š,è€Œæ¢æ¥çš„ç»“æœåˆæœ‰ç‚¹ä¸ç†æƒ³.æƒ³ç€æƒ³ç€è¿˜ä¸å¦‚é‡æ–°â€™é€ ä¸€ä¸ªâ€˜,åæ­£å·²ç»å®Œå…¨å¼„æ‡‚äº†å®ƒçš„åŸç†,å¹¶ä¸”è¿˜å¯ä»¥ä¸ºæœªæ¥å¢åŠ æ–°åŠŸèƒ½æˆ–è€…ç»´æŠ¤å¸¦æ¥è¯¸å¤šæ–¹ä¾¿,é‚£å°±é€ ä¸€ä¸ªå‘—,è¿™ä¸ªé˜¶æ®µç§°ä¹‹ä¸ºâ€œæ¨åˆ°é‡æ¥â€.

[* All-takenism: Call it the stage of "All-takenism", no matter what it is, use it first and then talk about it.]:#

[* Know the roots: After using it for nine years, I became more and more familiar with it, and I have a good understanding of some principles and problems, which is called the "knowing the roots" stage.]:#

[* Patch up: In the process of later use, there will always be some problems or some unsatisfactory functions. Since we know how it is done, we will try to change its problems or add functions. This stage is called "Patch up".]:#

[* Make a new start: Wait until it is completely understood, and find that simple modifications or adding functions can't change the underlying shortcomings. At this time, it is found that the patch time invested is more and more, and the results are a little different. Ideal. If you think about it, it might be better to 'build one' again. Anyway, you have fully understood its principle, and it can also bring a lot of convenience for adding new functions or maintenance in the future. Then build one. This stage is called "Make a new start".]:#

# Curator ç®€è¦æŒ‡å—

[# Introduction to Curator]:#

&nbsp;&nbsp;&nbsp;&nbsp; Apache Curator æ˜¯ä¸€ä¸ªç”¨äº Apache ZooKeeper çš„ Java/JVM å®¢æˆ·ç«¯åº“,å®ƒæ˜¯ä¸€ç§åˆ†å¸ƒå¼åè°ƒæœåŠ¡.å®ƒåŒ…æ‹¬ä¸€ä¸ªé«˜çº§ API æ¡†æ¶å’Œå®ç”¨ç¨‹åº,ä½¿ Apache ZooKeeper çš„ä½¿ç”¨æ›´åŠ è½»æ¾å’Œå¯é .å®ƒè¿˜åŒ…æ‹¬å¸¸è§ç”¨ä¾‹å’Œæ‰©å±•çš„ç§˜è¯€,ä¾‹å¦‚æœåŠ¡å‘ç°å’Œ Java 8 å¼‚æ­¥ DSL.

&nbsp;&nbsp;&nbsp;&nbsp; ä»¥è‡³äº Zookeeper çš„åˆ›å»ºè€… Patrick Hunt å¯¹ Curator æ¡†æ¶çš„è¯„ä»·æ˜¯: `Guava is to Java What Curator is to Zookeeper`. ç”±æ­¤å¯è§ Curator çš„ä¼˜ç§€ç¨‹åº¦.


[&nbsp;&nbsp;&nbsp;&nbsp; Apache Curator is a Java/JVM client library for Apache ZooKeeper, a distributed coordination service. It includes a highlevel API framework and utilities to make using Apache ZooKeeper much easier and more reliable. It also includes recipes for common use cases and extensions such as service discovery and a Java 8 asynchronous DSL.]:#

[&nbsp;&nbsp;&nbsp;&nbsp; So much so that Patrick Hunt, the creator of Zookeeper, commented on the Curator framework: `Guava is to Java What Curator is to Zookeeper`. This shows how excellent Curator is.]:#

## Curatorç®€ä»‹

[## Introduction to Curator]:#

* å°è£…å¹¶ç®€åŒ–äº†ZookeeperåŸç”ŸAPI

* è‡ªåŠ¨åŒ–è¿æ¥ç®¡ç†

* Watcheråå¤æ³¨å†Œ

* æµå¼ç¼–ç¨‹æ–¹å¼

* å¤±è´¥é‡è¯•æœºåˆ¶ 

* å¼ºå¤§çš„åˆ†å¸ƒå¼ç¯å¢ƒå·¥å…·èœè°±

* æ˜“æµ‹è¯•,å¿«é€Ÿå¯åŠ¨åµŒå…¥é›†ç¾¤

[* Wraps and simplifies Zookeeper native API]:#

[* Automated connection management]:#

[*  Watcher repeated registration]:#

[* Stream programming]:#

[* Failure retry mechanism]:# 

[* Powerful Distributed Environment Tools Recipe]:#

[* Easy to test, quickly start embedded cluster]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-3.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

* curator-frameworkï¼šå¯¹zookeeperçš„åº•å±‚apiçš„ä¸€äº›å°è£….

* curator-clientï¼šæä¾›ä¸€äº›å®¢æˆ·ç«¯çš„æ“ä½œ,ä¾‹å¦‚é‡è¯•ç­–ç•¥ç­‰.

* curator-recipesï¼šå°è£…äº†ä¸€äº›é«˜çº§ç‰¹æ€§,å¦‚ï¼šCacheäº‹ä»¶ç›‘å¬ã€é€‰ä¸¾ã€åˆ†å¸ƒå¼é”ã€åˆ†å¸ƒå¼è®¡æ•°å™¨ã€åˆ†å¸ƒå¼Barrierç­‰.

* curator-asyncï¼šCuratorå¼‚æ­¥æ“ä½œå°è£…ç±»,å°±åƒJava 8's lambdasè¡¨è¾¾å¼é‚£æ ·.

[* curator-framework: some encapsulation of zookeeper's underlying api.]:#

[* curator-client: Provides some client-side operations, such as retry strategies, etc.]:#

[* curator-recipes: encapsulates some advanced features, such as: Cache event monitoring, election, distributed lock, distributed counter, distributed Barrier, etc.]:#

[* curator-async: Curator wrapper class for asynchronous operations, just like Java 8's lambdas.]:#

## Curator ç‰ˆæœ¬å…¼å®¹

[## Curator Compatibility]:# 

&nbsp;&nbsp;&nbsp;&nbsp; Curator ç°åœ¨å¸‚é¢ä¸Šç”¨çš„æ¯”è¾ƒå¤šçš„ç‰ˆæœ¬æœ‰ `5.X`ã€`4.X`ã€`2.X`è¿™äº›.æ‰€æœ‰å…¸å‹åº”ç”¨åœºæ™¯éœ€è¦ä¾èµ–clientå’Œframework.

* 5.xæ”¯æŒ ZooKeeper 3.5å’Œ3.6(ä¼˜å…ˆ3.6),ç°åœ¨zookeeperæœ€åä¸€ä¸ªRelease æ˜¯3.6.2.

* 4.xæ”¯æŒ ZooKeeper 3.5å’Œ3.6,ç°åœ¨zookeeperæœ€åä¸€ä¸ªRelease æ˜¯3.5.8.

* 2.xæ”¯æŒ ZooKeeper 3.xåŠä»¥ä¸‹,ç°åœ¨zookeeperæœ€åä¸€ä¸ªRelease æ˜¯3.4.14

[&nbsp;&nbsp;&nbsp;&nbsp; There are many versions of Curator currently on the market, such as `5.X`, `4.X`, `2.X`. All typical application scenarios need to rely on client and framework.]:#

[* 5.x supports ZooKeeper 3.5 and 3.6 (3.6 is preferred), now the last release of zookeeper is 3.6.2.]:#

[* 4.x supports ZooKeeper 3.5 and 3.6, now the last release of zookeeper is 3.5.8.]:#

[* 2.x supports ZooKeeper 3.x and below, now the last release of zookeeper is 3.4.14.]:#

```
<dependency>
  <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>x.x</version>
</dependency>
```

## Curatorå¿«é€ŸæŒ‡å—ï¼ˆä¸€ï¼‰ï¼šåˆ›å»ºä¼šè¯

[## Curator Quick Guide (1): Creating a Session]:#

&nbsp;&nbsp;&nbsp;&nbsp; ä¸¤ç§æ–¹å¼ä½¿ç”¨,ä¸€ç§æ˜¯é€šè¿‡å·¥å‚æ–¹æ³•,ä¸€ç§æ˜¯é€šè¿‡æµå¼çš„builderæ–¹æ³•.

[&nbsp;&nbsp;&nbsp;&nbsp; It is used in two ways, one is through the factory method, the other is through the flow builder method]:#

### 1.Factory

```
CuratorFramework  curatorClient = CuratorFrameworkFactory .newClient(connectString,  sessionTimeoutMs,  connectionTimeoutMs, retryPolicy)
```

### 2.Stream

```
CuratorFramework curatorClient = CuratorFrameworkFactory.builder().
connectString(connectString).
sessionTimeoutMs(sessionTimeoutMs)
.connectionTimeoutMs(connectionTimeoutMs).
retryPolicy(retryPolicy).
build()
```

å‚æ•°è§£é‡Š:

connectString: zké›†ç¾¤è¿æ¥å­—ç¬¦ä¸²,æ ¼å¼host1:port1,host2:port2,host3:port3

connectionTimeoutMs: è¿æ¥è¶…æ—¶æ—¶é—´

sessionTimeoutMs: ä¼šè¯è¶…æ—¶æ—¶é—´

retryPolicy: å¤±è´¥é‡è¯•ç­–ç•¥


[Parameter explanation:]:#

[connectString: zk cluster connection string, format host1:port1,host2:port2,host3:port3]:#

[connectionTimeoutMs: connection timeout time]:#

[sessionTimeoutMs: session timeout]:#

[retryPolicy: Failed retry policy]:#

## Curatorå¿«é€ŸæŒ‡å—ï¼ˆäºŒï¼‰ï¼šå¯åŠ¨

[## Curator Quick Guide (2): Getting Started ]:#

Curatorä¼šè¯åˆ›å»ºå¥½ä¹‹å,å°±ç›´æ¥å¯åŠ¨,å¯åŠ¨æˆåŠŸä¹‹åå°±å¯ä»¥è¿›è¡Œå„ç§æ“ä½œ.

[After the Curator session is created, it will be started directly. Various operations can be performed after the startup is successful.]:#

```
curatorClient.start();

curatorClient.blockUntilConnected();
```

## Curatorå¿«é€ŸæŒ‡å—ï¼ˆä¸‰ï¼‰ï¼šç»“ç‚¹æ“ä½œ

[## Curator Quick Guide (3): Node Operations]:#

```
curatorClient.create().withMode(CreateMode.EPHEMERAL).forPath(â€œ/tempâ€);  //Create a temporary node

curatorClient.delete().forPath("/todelete");  //Delete node

curatorClient.setData().forPath("/update", "value".getBytes());  //Update Data

curatorClient.getChildren().forPath("/parentNodes");  // Get Nodes
```

## Curatorå¿«é€ŸæŒ‡å—ï¼ˆå››ï¼‰ï¼šRetryç­–ç•¥

[## Curator Quick Guide (4): Retry Strategy]:#

&nbsp;&nbsp;&nbsp;&nbsp; Curatoræ‰€æœ‰çš„æ“ä½œ,åŒ…æ‹¬setData create deleteç­‰ç­‰.å¦‚æœå¤±è´¥,éƒ½å…è®¸é‡è¯•ç³»ç»Ÿå†…éƒ¨äº†å‡ ç§é‡è¯•ç­–ç•¥.å¼€å‘äººå‘˜å¯ä»¥æ ¹æ®éœ€è¦,å®ç°RetryPolicyæ¥å£å®šåˆ¶è‡ªå·±çš„ç­–ç•¥.

[&nbsp;&nbsp;&nbsp;&nbsp; All operations of Curator, including setData create delete, etc. If it fails, it is allowed to retry several retry strategies within the system. Developers can customize their own strategies by implementing the RetryPolicy interface as needed.]:#

é»˜è®¤é‡è¯•ç­–ç•¥:

* RetryNTimes(int n, int sleepMsBetweenRetries)
n: æœ€å¤§é‡è¯•æ¬¡æ•°.
sleepMsBetweenRetry: é‡è¯•é—´éš”çš„æ—¶é—´.

* RetryOneTime(int sleepMsBetweenRetry)
sleepMsBetweenRetry: é‡è¯•é—´éš”çš„æ—¶é—´

* RetryUntilElapsed(int maxElapsedTimeMs, int sleepMsBetweenRetries)
é‡è¯•çš„æ—¶é—´è¶…è¿‡æœ€å¤§æ—¶é—´å,å°±ä¸å†é‡è¯•.
maxElapsedTimeMs: æœ€å¤§é‡è¯•æ—¶é—´.
sleepMsBetweenRetriees: æ¯æ¬¡é‡è¯•çš„é—´éš”æ—¶é—´.

* ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries, int maxSleepMs)
baseSleepTimeMs: åˆå§‹sleepæ—¶é—´.
maxRetries: æœ€å¤§é‡è¯•æ¬¡æ•°.
maxSleepMs: æœ€å¤§é‡è¯•æ—¶é—´.

å½“å‰åº”è¯¥ sleep çš„æ—¶é—´: baseSleepTimeMs*Math.max(1,random.nextInt(1 << retryCount+1)),éšç€é‡è¯•æ¬¡æ•°,å¢åŠ é‡è¯•æ—¶é—´é—´éš”,æŒ‡æ•°å¢åŠ .

[Default retry policy:]:#

[* RetryNTimes(int n, int sleepMsBetweenRetries)]:#
[n: maximum number of retries.]:#
[sleepMsBetweenRetry: time between retry.]:#

[* RetryOneTime(int sleepMsBetweenRetry)]:#
[sleepMsBetweenRetry: time between retry]:#

[* RetryUntilElapsed(int maxElapsedTimeMs, int sleepMsBetweenRetries)]:#
[After the retry time exceeds the maximum time, no more retries.]:#
[maxElapsedTimeMs: Maximum retry time.]:#
[sleepMsBetweenRetriees: The interval between each retry.]:#

[* ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries, int maxSleepMs)]:#
[baseSleepTimeMs: initial sleep time.]:#
[maxRetries: Maximum number of retries.]:#
[maxSleepMs: Maximum retry time.]:#

[The current time for sleep: baseSleepTimeMs*Math.max(1, random.nextInt(1 << retryCount+1)), with the number of retries, increase the retry interval, and the index increases.]:#

## Zkç»“ç‚¹æ“ä½œæ ¸å¿ƒAPI

[## Core API for Zk Node Operations]:#

&nbsp;&nbsp;&nbsp;&nbsp; Curatoræ‰€æœ‰çš„zkç»“ç‚¹æ“ä½œéƒ½æ˜¯é€šè¿‡Builderå®Œæˆ.æ¯ä¸€ç§æ“ä½œåå°å¯¹åº”ä¸€ä¸ªBuilder.è´Ÿè´£zkç»“ç‚¹çš„æ“ä½œ.
ç”±äºæ‰€æœ‰çš„æ“ä½œéƒ½é›·åŒ,ä¸‹é¢ä»¥`CreateBuilderImpl`ä¸ºä¾‹,è¯´æ˜å…¶å†…éƒ¨çš„å®ç°.

[&nbsp;&nbsp;&nbsp;&nbsp; All zk node operations of Curator are done through Builder. Each operation background corresponds to a Builder, which is responsible for the operation of the zk node.]:#
[Since all operations are the same, the following takes `CreateBuilderImpl` as an example to illustrate its internal implementation.]:#

| æ ¸å¿ƒAPI | å†…éƒ¨å®ç°æœºåˆ¶ | æè¿° |
| :--- | :---  | :---  |
| create() | CreateBuilderImpl | å¼€å§‹åˆ›å»ºæ“ä½œ, åœ¨æœ€åè°ƒç”¨`forPath()`æŒ‡å®šè¦æ“ä½œçš„ZNode |
| delete() | DeleteBuilderImpl | å¼€å§‹åˆ é™¤æ“ä½œ,åœ¨æœ€åè°ƒç”¨`forPath()`æŒ‡å®šè¦æ“ä½œçš„ZNode |
| checkExists() | ExistsBuilderImpl | å¼€å§‹æ£€æŸ¥ZNodeæ˜¯å¦å­˜åœ¨çš„æ“ä½œ, åœ¨æœ€åè°ƒç”¨`forPath()`æŒ‡å®šè¦æ“ä½œZNode |
| getData() | GetDataBuilderImpl | å¼€å§‹è·å¾—ZNodeèŠ‚ç‚¹æ•°æ®çš„æ“ä½œ,åœ¨æœ€åè°ƒç”¨`forPath()`æŒ‡å®šè¦æ“ä½œZNode |
| setData() | SetDataBuilderImpl | å¼€å§‹è®¾ç½®ZNodeèŠ‚ç‚¹æ•°æ®çš„æ“ä½œ,åœ¨æœ€åè°ƒç”¨`forPath()`æŒ‡å®šè¦æ“ä½œZNode |
| getChildren() | GetChildrenBuilderImpl | å¼€å§‹è·å¾—ZNodeçš„å­èŠ‚ç‚¹åˆ—è¡¨,åœ¨æœ€åè°ƒç”¨`forPath()`æŒ‡å®šè¦æ“ä½œZNode |

###### &nbsp;
[| Core API | Internal implementation | Describe |]:#
[| :--- | :---  | :---  |]:#
[| create() | CreateBuilderImpl | Start creating the operation, and call f`orPath()` at the end to specify the ZNode to operate |]:#
[| delete() | DeleteBuilderImpl | Start the delete operation, Call `forPath()` at the end to specify the ZNode to be operated on |]:#
[| checkExists() | ExistsBuilderImpl | Begin the operation of checking whether the ZNode exists,At the end, call `forPath()` to specify that the ZNode is to be operated]:#
[| getData() | GetDataBuilderImpl | Start the operation to get the data of the ZNode node. At the end, call `forPath()` to specify that the ZNode is to be operated |]:#
[| setData() | SetDataBuilderImpl | Start the operation of setting the ZNode node data. At the end, call `forPath()` to specify the ZNode to be operated |]:#
[| getChildren() | GetChildrenBuilderImpl | Start getting the list of child nodes of ZNode, Call `forPath()` at the end to specify the ZNode to be manipulated |]:#

# Curator åº”ç”¨åœºåˆ

[# Curator Application Scenario]:#

## åˆ†å¸ƒå¼åŸå­å€¼

[## Distributed Atomic Values]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-4.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
åŒä¸€ä¸ªè¿›ç¨‹å†…,å¤šä¸ªçº¿ç¨‹é€šè¿‡javaå¹¶å‘åŒ…æä¾›çš„åŸå­å€¼è¿›è¡Œæ“ä½œ,ä¿è¯äº†å¤šä¸ªçº¿ç¨‹çš„å¹¶å‘åŸå­æ€§.
</center>

[In the same process, multiple threads operate through the atomic value provided by the java concurrent package, ensuring the concurrent atomicity of multiple threads]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-5.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
ä¸åŒè¿›ç¨‹æ— æ³•åˆ©ç”¨jdkçš„å¹¶å‘åŒ…çš„åŸå­å€¼.Curatorçš„atomicæä¾›äº†ç±»ä¼¼çš„åŠŸèƒ½,åœ¨åˆ†å¸ƒå¼ç¯å¢ƒå†…å…±äº«åŒä¸€ä¸ªåŸå­å€¼.
</center>

[Different processes cannot use the atomic value of jdk's concurrent packets. Curator's atomic provides a similar function, sharing the same atomic value in a distributed environment.]:#

## åˆ†å¸ƒå¼åŸå­å€¼ä»£ç ç‰‡æ®µå®ä¾‹

[## Distributed Atomic Value Code Snippet Example]:#

å¤šä¸ªåˆ†å¸ƒåœ¨ä¸åŒæœºå™¨,å¯¹åŒä¸€ä¸ªåŸå­å˜é‡è¿›è¡Œå¹¶å‘æ“ä½œ,å…¶å®ƒæœºå™¨éƒ½èƒ½çœ‹åˆ°æœ€æ–°çš„å€¼.

[Multiple distributions on different machines perform concurrent operations on the same atomic variable, All other machines can see the latest value.]:#

```
RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10);
CuratorFramework cf = CuratorFrameworkFactory.builder().connectString(CONNECT_ADDR).sessionTimeoutMs(SESSION_OUTTIME).retryPolicy(retryPolicy).build();
cf.start();

DistributedAtomicInteger atomicIntger = new DistributedAtomicInteger(cf, "/atomicValue",new RetryNTimes(3, 1000));
System.out.println(atomicIntger.get().preValue()); 
AtomicValue<Integer> value = atomicIntger.add(1);
System.out.println(value.postValue()); 
```

## åˆ†å¸ƒå¼é”

[## Distributed Lock]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-6.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
åŒä¸€ä¸ªè¿›ç¨‹å†…,å¤šä¸ªçº¿ç¨‹é€šè¿‡javaå¹¶å‘åŒ…æä¾›çš„é”,ä¿è¯å¤šçº¿ç¨‹çš„å…±äº«é”.
</center>

[In the same process, multiple threads use the lock provided by java concurrent package to ensure the shared lock of multiple threads.]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-7.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
ä¸åŒè¿›ç¨‹æ— æ³•åˆ©ç”¨jdkçš„å¹¶å‘åŒ…çš„é”,Curatorçš„locksæä¾›äº†ç±»ä¼¼çš„åŠŸèƒ½,åœ¨åˆ†å¸ƒå¼ç¯å¢ƒå†…è¿›è¡Œå…¨å±€é”çš„å¤„ç†.
</center>

[Different processes cannot use the locks of JDK's concurrent packets. Curator's locks provide a similar function to process global locks in a distributed environment.]:#

## åˆ†å¸ƒå¼é”ä»£ç ç‰‡æ®µå®ä¾‹

[## Distributed lock code snippet]:#

å¤šä¸ªåˆ†å¸ƒåœ¨ä¸åŒæœºå™¨,åªè¦å¾—åˆ°é”æ‰èƒ½æ‰§è¡Œå¾ªç¯é‡Œé¢çš„ä»£ç ,å› æ­¤å°±èƒ½é¿å…é‡å¤æ‰§è¡Œçš„é€»è¾‘.

[Multiple distributed in different machines, the code in the loop can be executed only if the lock is obtained, so the logic of repeated execution can be avoided.]:#

```
RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10);
CuratorFramework cf = CuratorFrameworkFactory.builder().connectString(CONNECT_ADDR)
			.sessionTimeoutMs(SESSION_OUTTIME).retryPolicy(retryPolicy).build();
cf.start();
InterProcessMutex lock = new InterProcessMutex(cf, "/lock");

try {
	lock.acquire();
	for (int i = 0; i < 1000000; i++) {
		SimpleDateFormat sdf = new SimpleDateFormat("HH:mm:ss|SSS");
		System.out.println(sdf.format(new Date()));
		TimeUnit.SECONDS.sleep(1);
}
} finally {
	lock.release();
}
```

## ä¸»ä»é€‰ä¸¾ç‰‡æ®µå®ä¾‹

[## Master-slave election fragment instance]:#

&nbsp;&nbsp;&nbsp;&nbsp; åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡ä¸­ï¼Œé€‰ä¸»æ˜¯ä¸€ä¸ªå¸¸è§çš„åœºæ™¯ã€‚é€‰ä¸»æ˜¯ä¸€ä¸ªè¿™æ ·çš„è¿‡ç¨‹ï¼Œé€šè¿‡é€‰ä¸»ï¼Œä¸»èŠ‚ç‚¹è¢«é€‰æ‹©å‡ºæ¥æ§åˆ¶å…¶ä»–èŠ‚ç‚¹æˆ–è€…æ˜¯åˆ†é…ä»»åŠ¡ã€‚
é€‰ä¸»ç®—æ³•è¦æ»¡è¶³çš„å‡ ä¸ªç‰¹å¾ï¼š
1. å„ä¸ªèŠ‚ç‚¹å‡è¡¡çš„è·å¾—æˆä¸ºä¸»èŠ‚ç‚¹çš„æƒåˆ©ï¼Œä¸€æ—¦ä¸»èŠ‚ç‚¹è¢«é€‰å‡ºï¼Œå…¶ä»–çš„èŠ‚ç‚¹å¯ä»¥æ„ŸçŸ¥åˆ°è°æ˜¯ä¸»èŠ‚ç‚¹
2. ä¸»èŠ‚ç‚¹æ˜¯å”¯ä¸€å­˜åœ¨çš„
3. ä¸€æ—¦ä¸»èŠ‚ç‚¹å¤±æ•ˆï¼Œå®•æœºæˆ–è€…æ–­å¼€è¿æ¥ï¼Œå…¶ä»–çš„èŠ‚ç‚¹èƒ½å¤Ÿæ„ŸçŸ¥ï¼Œå¹¶ä¸”é‡æ–°è¿›è¡Œé€‰ä¸»ç®—æ³•ã€‚

[&nbsp;&nbsp;&nbsp;&nbsp; In distributed system design, master election is a common scenario. Master election is a process by which the master node is selected to control other nodes or assign tasks.]:#
[Several characteristics to be satisfied by the main election algorithm:]:#
[1. Each node obtains the right to become the master node in a balanced manner.Once the master node is selected, other nodes can perceive who is the master node. ]:#
[2. The master node is the only one that exists]:#
[3. Once the master node fails, goes down or disconnects, other nodes can sense it and re-select the master algorithm.]:#

```
RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10);
CuratorFramework cf = CuratorFrameworkFactory.builder().connectString(CONNECT_ADDR)
			.sessionTimeoutMs(SESSION_OUTTIME).retryPolicy(retryPolicy).build();
cf.start();

LeaderLatch leaderLatch = new LeaderLatch(cf, "/latch");
leaderLatch.start();
leaderLatch.await(10, TimeUnit.SECONDS);
if (leaderLatch.hasLeadership()) {
	System.out.println("yes, i am leader");
}
leaderLatch.close();
```

## åˆ†å¸ƒå¼æ …æ  DistributedBarrier

[## Distributed DistributedBarrier]:#

å¿…é¡»ç­‰å…¨éƒ¨ä»»åŠ¡å°±ç»ªä¹‹å,æ‰èƒ½å¯åŠ¨äº‹åŠ¡å¤„ç†.åœ¨å•è¿›ç¨‹ä¸­,java.util.concurrentåŒ…æä¾›äº†Barrier.é‚£ä¹ˆå‡å¦‚æ˜¯æ§åˆ¶æ‰€æœ‰åˆ†å¸ƒåœ¨ä¸åŒæœºå™¨çš„è¿›ç¨‹æˆ–çº¿ç¨‹å‘¢ï¼Ÿ

Curatorä¸ºæˆ‘ä»¬æä¾›å…¨å±€çš„åˆ†å¸ƒå¼æ …æ .

[You must wait for all tasks to be ready before you can start transaction processing. In a single process, the java.util.concurrent package provides Barrier. So what if you control all processes or threads distributed across different machines?]:#

[Curator provides us with a global distributed fence.]:#



## åˆ†å¸ƒå¼æ …æ DistributedBarrierä»£ç ç‰‡æ®µ

[## Distributed DistributedBarrier code snippet]:#

```
RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10);
CuratorFramework cf = CuratorFrameworkFactory.builder().connectString(CONNECT_ADDR)
		.sessionTimeoutMs(SESSION_OUTTIME).retryPolicy(retryPolicy).build();
cf.start();
ExecutorService service = Executors.newFixedThreadPool(10);
DistributedBarrier controlBarrier = new DistributedBarrier(cf, "/barrier");
controlBarrier.setBarrier();

for (int i = 0; i < 10; ++i) {
final int index = i;
Callable<Void> task = () -> {
	Thread.sleep((long) (3 * Math.random()));				controlBarrier.waitOnBarrier();
	System.out.println("Client #" + index + " begins");
	return null;
	};
service.submit(task);
}
Thread.sleep(10000);
System.out.println("all Barrier instances should wait the condition");
controlBarrier.removeBarrier();
service.shutdown();
```

# Curator å†…éƒ¨å‰–æ

[# Curator Internal Anatomy]:#

## è¶…æ—¶æ£€æŸ¥å’Œé‡è¯•æœºåˆ¶ï¼šä»¥åˆ›å»ºç»“ç‚¹ä¸ºä¾‹

[## Timeout Check and Retry Mechanism: Take Creating Nodes as an Example]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-8.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

å¤‡æ³¨:

å…¶ä¸­è¶…æ—¶æ—¶é—´ä¸ºåˆ›å»ºCuratorå®¢æˆ·ç«¯çš„æ—¶å€™è®¾ç½®çš„connectionTimeoutMs.

Curatorä¸­ä»»ä½•çš„æ“ä½œéƒ½éœ€è¦ç­‰å¾…è¿æ¥å®Œæˆå°±æ˜¯ç­‰å¾…connectionTimeoutMsçš„æ—¶é—´,å¦‚æœåœ¨è¯¥æ—¶é—´æ²¡è¿ä¸Š,åˆ™æ“ä½œå¤±è´¥,é‚£ä¹ˆæ“ä½œæ˜¯å¦ç»§ç»­å°±è¦æ ¹æ®Retryç­–ç•¥.

Backgroundçš„å¼‚æ­¥createç”±äºæ—¶é—´ç¯‡å¹…å…³ç³»,ä¸å±•å¼€è®²,å…·ä½“å®ç°ç±»ä¼¼.

[Remark:]:#

[The timeout is the connectionTimeoutMs set when the Curator client was created.]:#

[Any operation in Curator needs to wait for the connection to complete, which is the time to wait for connectionTimeoutMs. If it is not connected within this time, the operation fails. Then whether the operation continues depends on the Retry policy.]:#

[The asynchronous create of Background will not be discussed due to the time and space. The specific implementation is similar.]:#

## ä¼šè¯è¶…æ—¶å’Œä¼šè¯é‡å»º

[## Session timeout and session rebuild]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-9.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

`sessionTimeoutMs`åˆ™æ˜¯æŒ‡å½“å‰å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨æ–­å¼€è¶…æ—¶çš„æ—¶é—´.å½“å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨æ–­è¿çš„æ—¶é—´è¶…è¿‡è¯¥æ—¶é—´,ä¼šè¯å°†`Expired`.

ä¼šè¯è¶…æ—¶,é‚£ä¹ˆå­˜å‚¨åœ¨`ZK`ä¸Šçš„æ‰€æœ‰ä¸´æ—¶æ•°æ®ä¸æ³¨å†Œçš„è®¢é˜…è€…éƒ½ä¼šè¢«ç§»é™¤,æ­¤æ—¶éœ€è¦é‡æ–°åˆ›å»ºä¸€ä¸ªZooKeeperå®¢æˆ·ç«¯å®ä¾‹,éœ€è¦è‡ªå·±ç¼–ç åšä¸€äº›é¢å¤–çš„å¤„ç†.

å¹¸è¿çš„æ˜¯,`Curator`ä¼šå¸®æˆ‘ä»¬åšå¦‚ä¸‹çš„äº‹æƒ…:
A.å…ˆå…³é—­æ—§çš„`zookeeper`å®¢æˆ·ç«¯,B.è·å–è¿æ¥ä¸²,é€šè¿‡`zookeeperFactory`å·¥å‚é‡æ–°åˆ›å»ºæ–°çš„`Zookeeper`å®¢æˆ·ç«¯.

sessionTimeoutMsä¸º30ç§’.

ç¬¬10ç§’ping server.

ç¬¬20ç§’å¼€å§‹å°è¯•é‡è¿å…¶å®ƒæœåŠ¡å™¨.

ç¬¬29ç§’åé‡è¿ä¸Š,é‚£ä¹ˆè¯¥sessionè¿˜æœ‰æ•ˆ.

ç¬¬31ç§’é‡è¿ä¸Šä¹‹å,è¯¥Sessionå°†æ ‡è®°ä¸ºExpired,Curatorå¸®æˆ‘ä»¬é‡å»ºä¼šè¯,NodeCache/TreeCacheç­‰ç›‘å¬å™¨ä¾ç„¶æœ‰æ•ˆ,ä½†æ˜¯ä¸€æ¬¡æ€§æ¶ˆè´¹çš„Watcherå°†å¤±æ•ˆã€‚

[`sessionTimeoutMs` refers to the current client and server disconnection timeout time. When the client and server are disconnected for longer than this time, the session will be `Expired`.]:#

[If the session times out, all temporary data stored on `ZK` and registered subscribers will be removed. In this case, a ZooKeeper client instance needs to be recreated, and some additional processing needs to be coded by yourself.]:#

[Fortunately, `Curator` does the following for us:]:#
[A. First close the old zookeeper client,B. Get the connection string and recreate the new `zookeeper` client through the `zookeeperFactory` factory.]:#

[sessionTimeoutMs is 30 seconds.]:#

[10 seconds ping server.]:#

[At 20 seconds, try to reconnect to other servers.]:#

[If you reconnect after 29 seconds, the session is still valid.]:#

[After the 31st second reconnection, the session will be marked as Expired. Curator helps us rebuild the session. Listeners such as NodeCache/TreeCache are still valid. But the one-time consumption Watcher will be invalid.]:#

## Curatoräº‹ä»¶ç›‘å¬

äº‹ä»¶ç›‘å¬æ˜¯Zookeeperæœ€æ ¸å¿ƒåŠŸèƒ½,ä¹Ÿæ˜¯Curatorçš„æ ¸å¿ƒåŠŸèƒ½.ç¦»å¼€äº†äº‹ä»¶ç›‘å¬,zooekeeperä»€ä¹ˆä¹Ÿä¸æ˜¯.

ç”±äºzookeeperåŸç”Ÿå¤„ç†æ˜¯ä¸€æ¬¡æ€§æ¶ˆè´¹,éå¸¸ä¸æ–¹ä¾¿.
Curatorå¯¹Zookeeperäº‹ä»¶å¤„ç†åšäº†å°è£…,ä¸»è¦å¦‚ä¸‹:

* ConnectionStateListener: ç”Ÿå‘½å‘¨æœŸäº‹ä»¶.
* Watcher: CuratorWatcher.
* NodeCache: ç›‘å¬ç»“ç‚¹æœ¬èº«å†…å®¹å˜åŒ–å’Œç»“ç‚¹çš„å¢åˆ äº‹ä»¶.
* TreeCache: ç›‘å¬ç»“ç‚¹æœ¬èº«çš„å¢åˆ æ”¹ä»¥åŠå­ç»“ç‚¹çš„å¢åˆ æ”¹.

[## Curator event listener]:#

[Event monitoring is the core function of Zookeeper and the core function of Curator. Without event listeners, zooekeeper is nothing.]:#

[Since the native processing of zookeeper is a one-time consumption, it is very inconvenient.]:#
[Curator encapsulates Zookeeper event processing, mainly as follows:]:#

[* ConnectionStateListenerï¼šLifecycle Events.]:#
[* Watcher: CuratorWatcher.]:#
[* NodeCacheï¼šMonitor the content changes of the node itself and the addition and deletion events of the node.]:#
[* TreeCacheï¼šMonitor the additions, deletions and changes of the node itself and the additions, deletions and changes of child nodes.]:#

## Curatorç”Ÿå‘½å‘¨æœŸäº‹ä»¶: ConnectionStateListener

[## Curator life cycle events: ConnectionStateListener]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-10.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

CONNECTED:æ•´ä¸ªç”Ÿå‘½å‘¨æœŸåªä¼šè¿›å…¥ä¸€æ¬¡.

SUSPENDED:æ¯æ¬¡ä¸­æ–­éƒ½ä¼šè¿›å…¥æ­¤çŠ¶æ€.

RECONNECTED:é‡æ–°è¿æ¥æˆåŠŸ.

LOST:å½“é‡è¿è¶…æ—¶æˆ–è€…Sessionè¶…æ—¶çš„æ—¶å€™.å½“ä¸‹åˆ—çš„æƒ…å†µä¸‹ä¼šå‡ºç°LOSTäº‹ä»¶:

* é‡è¿çš„æ—¶å€™connectionTimeoutMsè¶…æ—¶
* é‡è¿çš„æ—¶å€™è¶…è¿‡äº†sessionTimeoutMsè¶…æ—¶


æ³¨æ„:ç”±äºcurator3.0ç‰ˆæœ¬ä»¥ä¸‹æœ‰ä¸ªbug,å‡å¦‚connectionTimeoutMså’ŒsessionTimeoutMsä¸€æ ·çš„æƒ…å†µ,éœ€è¦è¶…è¿‡ä¸¤å€æ—¶é—´,æ‰èƒ½æ”¶åˆ°LOSTçŠ¶æ€å’Œäº‹ä»¶.

å…³äºLOSTäº‹ä»¶çš„æŠ›å‡º,å¯å‚è€ƒCuratorFrameworkImpl.doSyncForSuspendedConnectionæ–¹æ³•.

[CONNECTED: Entire lifetime will only be entered once.]:#

[SUSPENDED: This state is entered on every interrupt.]:#

[RECONNECTED: reconnected successfully.]:#

[LOST: When the reconnection times out or the Session times out. The LOST event occurs in the following cases:]:#

[* connectionTimeoutMs times out when reconnecting.]:#
[* The sessionTimeoutMs timeout exceeded when reconnecting.]:#

[Note: Due to a bug in curator version 3.0 and below, if connectionTimeoutMs is the same as sessionTimeoutMs, it will take more than twice the time to receive LOST status and events.]:#

[For the throwing of the LOST event, please refer to CuratorFrameworkImpl.doSyncForSuspendedConnection method.]:#

## NodeCacheäº‹ä»¶ç›‘å¬

[## NodeCache event listener]:#

NodeCacheæ¯”è¾ƒç®€å•,åªç›‘å¬å½“å‰ç»“ç‚¹çš„å˜åŒ–.

[NodeCache is relatively simple, and only listens for changes in the current node.]:#

| äº‹ä»¶ | æ“ä½œ | æ“ä½œ | æ“ä½œ |
| :--- | :---  | :---  |:---  |
| nodeChangedäº‹ä»¶ | å½“å‰ç»“ç‚¹åˆ›å»º | å½“å‰ç»“ç‚¹åˆ é™¤ | å½“å‰ç»“ç‚¹å†…å®¹æ›´æ”¹ |

###### &nbsp;
[| Event | Operate | Operate | Operate |]:#
[| :--- | :---  | :---  |:---  |]:#
[| nodeChanged Event | Create the current node | Delete the current node | Changes the Current node |]:#

## NodeCacheäº‹ä»¶å®ç°åŸç†

[## NodeCache event implementation principle]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-11.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

## TreeCacheäº‹ä»¶ç›‘å¬

[## TreeCache event listener]:#

TreeCacheç›‘å¬å½“å‰ç»“ç‚¹çš„å˜åŒ–(å¢åˆ æ”¹),ä»¥åŠå­èŠ‚ç‚¹çš„å˜åŒ–(å¢åˆ æ”¹).

[TreeCache listens to the changes of the current node (Create/Delete/Change), as well as the changes of child nodes (Create/Delete/Change).]:#

| èŠ‚ç‚¹äº‹ä»¶ | å½“å‰ç»“ç‚¹å¢åŠ  | å½“å‰ç»“ç‚¹åˆ é™¤ | å½“å‰èŠ‚ç‚¹ä¿®æ”¹ | å­ç»“ç‚¹å¢åŠ  | å­ç»“ç‚¹åˆ é™¤ | å­èŠ‚ç‚¹ä¿®æ”¹ |
| :--- | :---  | :---  |:---  |:---  |:---  |:---  |
| NODE_ADDED | ğŸ‘Œ |  |  | ğŸ‘Œ |  |  |
| NODE_REMOVED |  | ğŸ‘Œ |  |  | ğŸ‘Œ |  |
| NODE_UPDATED |  |  | ğŸ‘Œ |  |  | ğŸ‘Œ |

###### &nbsp;
[| Node Event | Create the current node | Delete the current node | Change the current node | Create the child node | Delete the child node | Change the child node |]:#
[| :--- | :---  | :---  |:---  |:---  |:---  |:---  |]:#
[| NODE_ADDED | ğŸ‘Œ |  |  | ğŸ‘Œ |  |  |]:#
[| NODE_REMOVED |  | ğŸ‘Œ |  |  | ğŸ‘Œ |  |]:#
[| NODE_UPDATED |  |  | ğŸ‘Œ |  |  | ğŸ‘Œ |]:#

## TreeCacheäº‹ä»¶åŸç†

[## TreeCache event principle]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-12.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

maxDepthç›‘å¬å¯ä»¥æ§åˆ¶ç›‘å¬æ ‘çš„å“ªä¸€çº§èŠ‚ç‚¹.

0 åªç›‘å¬æœ¬èŠ‚ç‚¹

1 ç›‘å¬æœ¬èŠ‚ç‚¹+å­èŠ‚ç‚¹

ä»¥æ­¤ç±»æ¨.

[maxDepth monitoring can control the monitoring tree which level node.]:#

[0 only listen to this node]:#

[1 Monitor this node + child nodes]:#

[and so on]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-13.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

# Curator æœ€ä½³å®è·µ

[# Curator Best Practices]:#

## æœ€ä½³å®è·µ1ï¼šä½¿ç”¨æµå¼ç¼–ç¨‹æ¨¡å¼

[## Best Practice 1: Use Streaming Programming Patterns]:#

```
CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder().connectString("localhost:2181").sessionTimeoutMs(30000).connectionTimeoutMs(5000).retryPolicy(new ExponentialBackoffRetry(5000, 3, 10000)). namespace("a.b.c");
CuratorFramework curatorFramework = builder.build();

curatorFramework.create().creatingParentContainersIfNeeded().withMode(CreateMode.EPHEMERAL).forPath("/a/b/c");
```

## æœ€ä½³å®è·µ2ï¼šä¿å­˜å¹¶å¤ç”¨CuratorFrameImplå®¢æˆ·ç«¯

[## Best Practice 2: Save and reuse the CuratorFrameImpl client]:#

CuratorFrameImplå°±æ˜¯å¯¹zookeeperå®¢æˆ·ç«¯çš„ä¸€ä¸ªåŒ…è£…,åˆ›å»ºå’Œé”€æ¯éƒ½æ¯”è¾ƒæ˜‚è´µ.åˆ›å»ºå¥½ä¹‹åå¯ä»¥å¤ç”¨,ç”šè‡³å…¨å±€å…±ç”¨ä¸€ä¸ªå®ä¾‹.

[CuratorFrameImpl is a wrapper around the zookeeper client, which is expensive to create and destroy.After it is created, it can be reused, or even share an instance globally.]:#

## æœ€ä½³å®è·µ3ï¼šå–„ç”¨å¤„ç†è·¨æœºå™¨çš„åˆ†å¸ƒå¼å¹¶å‘åŒæ­¥

[## Best Practice 3: Take Advantage of Distributed Concurrent Objects]:#

Curatoræä¾›çš„Locksã€Barrierã€Queueã€Leaderèƒ½å¤Ÿè§£å†³å¾ˆå¤šè·¨æœºå™¨çš„åˆ†å¸ƒå¼åŒæ­¥é—®é¢˜.

[The Locks, Barrier, Queue, and Leader provided by Curator can solve many distributed synchronization problems across machines.]:#

## æœ€ä½³å®è·µ4ï¼šåˆç†è®¾ç½®connectonTimeoutMså’ŒsessionTimeoutMs

[## Best practice 4: Set connectonTimeoutMs and sessionTimeoutMs reasonably]:#

1. connectionTimeoutMså’ŒsessionTimeoutMsæ ¹æ®é›†ç¾¤ç½‘ç»œçš„æƒ…å†µè®¾ç½®,ä¸å®œå¤ªå¤§,ä¹Ÿä¸å®œå¤ªå°.

2. connectionTimeoutMsè®¾ç½®è¦æ¯”sessionTimeoutMså°,connectionTimeoutMsæ¯”sessionTimeoutMsæ²¡æ„ä¹‰.

[1. connectionTimeoutMs and sessionTimeoutMs are set according to the cluster network conditions. Neither too big nor too small.]:#

[2. The connectionTimeoutMs setting is smaller than the sessionTimeoutMs. connectionTimeoutMs is less meaningful than sessionTimeoutMs.]:#

## æœ€ä½³å®è·µ5ï¼šç›‘å¬sessionExpiredäº‹ä»¶ã€‚ä¼šè¯è¶…æ—¶ï¼Œé‡æ–°è¿ä¸Šï¼Œéœ€è¦é‡å»ºä¼šè¯

[## Best Practice 5: Listen to the sessionExpired event]:#

Curatorçš„Sessionä¸€æ—¦è¶…æ—¶,zkæœåŠ¡å™¨å°†æ¸…é™¤æ‰€æœ‰ç›‘å¬å™¨,å¹¶ä¼šç«‹åˆ»åˆ é™¤empmeralç»“ç‚¹.

å¦‚æœé‡æ–°æˆåŠŸ,æœåŠ¡å™¨å‘ç°å·²ç»è¶…è¿‡äº†sessionè®¾ç½®çš„è¶…æ—¶æ—¶é™,é‚£ä¹ˆå®¢æˆ·ç«¯å°†ä¼šæ”¶åˆ°ä¸€ä¸ªExpired event,è¡¨ç¤ºä¼šè¯å·²ç»ç»ˆæ­¢,éœ€è¦é‡å»ºå®¢æˆ·ç«¯.Curatorå·²ä¸ºæˆ‘ä»¬åšå¥½è¿™äº›.

ä½†æ˜¯éœ€è¦æˆ‘ä»¬è‡ªå·±é‡æ–°åˆ›å»ºç›¸åº”çš„ä¸´æ—¶ç»“ç‚¹,å¹¶é‡æ–°æ³¨å†ŒWatcherç›‘å¬ å™¨(éNodeCache/TreeCacheç›‘å¬å™¨ï¼‰ç­‰ç­‰.

[Once the Curator's Session times out, the zk server will clear all listeners and delete the emperal node immediately.]:#

[If successful again. The server finds that the timeout period set by the session has been exceeded, then the client will receive an Expired event, indicating that the session has been terminated and the client needs to be rebuilt. Curator has done this for us.]:#

[But we need to recreate the corresponding temporary nodes ourselves, and re-register the Watcher listeners (non-NodeCache/TreeCache listeners) and so on.]:#

## æœ€ä½³å®è·µ6ï¼šè°¨æ…ä½¿ç”¨LOSTäº‹ä»¶

[## Best practice 6: Use LOST events with caution]:#

LOSTå¤§éƒ¨åˆ†ä¸ºå®¢æˆ·ç«¯ä¸zkæœåŠ¡ç«¯è¿æ¥è¶…æ—¶,å¹¶éSessionè¶…æ—¶.æ”¶åˆ°LOSTäº‹ä»¶,ä¸ä¸€å®šä»£è¡¨Sessionè¶…æ—¶.å¾ˆå¤šäººéƒ½ä¼šè¯¯ç”¨,åŒ…æ‹¬ç½‘ä¸Šçš„å¾ˆå¤šæ–‡æ¡£.LOSTä¸ºCuratorå®¢æˆ·ç«¯å‘é€å‡ºæ¥çš„äº‹ä»¶,Session_Expiredä¸ºæœåŠ¡å™¨å‘ç»™å®¢æˆ·ç«¯çš„äº‹ä»¶,ä¸¤è€…ä¸è¦æ··æ·†.

[Most of the LOST is the connection timeout between the client and the zk server, not the session timeout. The received LOST event does not necessarily mean the session timeout. Many people misuse it, including many documents on the Internet. LOST is an event sent by the Curator client. Session_Expired is an event sent by the server to the client, and the two should not be confused.]:#

## æœ€ä½³å®è·µ7ï¼šä¼˜å…ˆä½¿ç”¨NodeCache/TreeCacheï¼Œè€Œä¸æ˜¯Watcher

[## Best Practice 7: Prefer NodeCache/TreeCache over Watcher]:#

Watcher æ˜¯ä¸€æ¬¡æ€§æ¶ˆè´¹,æ¶ˆè´¹ä¹‹åå¿…é¡»é‡æ–°æ³¨å†Œ,å®¹æ˜“å‡ºé”™.é€šè¿‡ NodeCache/TreeCache ,è®© Curator ä¸ºæˆ‘ä»¬ç®¡ç†ç›‘å¬å™¨.åŒ…æ‹¬æ–­å¼€ReConnected/Sessionè¶…æ—¶ç­‰,éƒ½ä¼šæ³¨å†Œç›‘å¬å™¨.

[Watcher is a one-time consumption, and it must be re-registered after consumption, which is prone to errors. Through NodeCache/TreeCache, let Curator manage the listener for us. Including disconnection of ReConnected/Session timeout, etc., the listener will be registered.]:#

## ä»‹ç»æœ€ä½³å®è·µ8ä¹‹å‰,å…ˆé’ˆå¯¹ä¸€ä¸ªåœºæ™¯æå‡ºä¸€ä¸ªç–‘é—®.

[## Before introducing Best Practice 8, let's ask a question about a scenario.]:#

<center>
<img src="../assets/images/curator-internal-implementation-mechanism/figure-14.png" alt="Curator internal implementation mechanism" title="Github of Anigkus" >
</center>

è®¾æƒ³ä¸€ä¸ªåœºæ™¯:

1. å®¢æˆ·ç«¯åˆ›å»ºä¸€ä¸ªé¡ºåºç»“ç‚¹00000000.
2. æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ,å¹¶ä¸”æœåŠ¡å™¨é›†ç¾¤äº‹åŠ¡æäº¤æˆåŠŸ.
3. è¿”å›ç»™å®¢æˆ·ç«¯.
4. æ•°æ®åŒ…å‘é€åˆ°çº¿è·¯ä¹‹å‰ç½‘ç»œå¼‚å¸¸æ–­å¼€,æ­¤æ¬¡å®¢æˆ·ç«¯è¯·æ±‚å°±ä¼šæ‰§è¡Œå¤±è´¥.é‚£ä¹ˆå®¢æˆ·ç«¯æ— æ³•ç¡®å®šæ˜¯æˆåŠŸè¿˜æ˜¯å¤±è´¥.
5. Curatorä¼šä¸ºæˆ‘ä»¬åœ¨æ­¤é‡è¯•,é‚£ä¹ˆå°±ä¼šåˆ›å»º00000001.

æ˜¾ç„¶è¿™ä¸æ˜¯æˆ‘ä»¬è¦çš„ç»“æœã€‚

[Consider a scenario:]:#

[1. The client creates a sequence node 00000000.]:#
[2. The server was created successfully, and the server cluster transaction was submitted successfully.]:#
[3. Return to the client.]:#
[4. The network is abnormally disconnected before the data packet is sent to the line, and the client request will fail to execute this time. Then the client cannot determine success or failure.]:#
[5. Curator will retry for us here, then it will create 00000001.]:#

[Obviously this is not the result we want]:#

## æœ€ä½³å®è·µ8: åˆ›å»ºé¡ºåºç»“ç‚¹ä½¿ç”¨withProtection

[## Best Practice 8: Use withProtection to create sequential nodes]:#

```
curatorFramework.create().creatingParentsIfNeeded().withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(path);
```

CreateBuilder æä¾›äº†ä¸€ä¸ª withProtection æ–¹æ³•æ¥é€šçŸ¥Curatorå®¢æˆ·ç«¯,åœ¨åˆ›å»ºçš„æœ‰åºèŠ‚ç‚¹å‰æ·»åŠ ä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦,å¦‚æœcreateæ“ä½œå¤±è´¥äº†,å®¢æˆ·ç«¯å°±ä¼šå¼€å§‹é‡è¯•æ“ä½œ,è€Œé‡è¯•æ“ä½œçš„ä¸€ä¸ªæ­¥éª¤å°±æ˜¯éªŒè¯æ˜¯å¦å­˜åœ¨ä¸€ä¸ªèŠ‚ç‚¹åŒ…å«è¿™ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦.

Curatorå®¢æˆ·ç«¯ä¸­æä¾›äº†ä¸€ä¸ªæ–¹æ³•,å¯¹åº”ç”¨ç¨‹åºçš„deleteæ“ä½œçš„æ‰§è¡Œæä¾›äº†ä¿éšœ,Curatorå®¢æˆ·ç«¯ä¼šé‡æ–°æ‰§è¡Œæ“ä½œ,ç›´åˆ°æˆåŠŸä¸ºæ­¢,æˆ–Curatorå®¢æˆ·ç«¯å®ä¾‹ä¸å¯ç”¨æ—¶.ä½¿ç”¨è¯¥åŠŸèƒ½,åªéœ€è¦ä½¿ç”¨DeleteBuilderæ¥å£ä¸­å®šä¹‰çš„ guaranteed æ–¹æ³•.

[CreateBuilder provides a withProtection method to notify the Curator client to add a unique identifier before the created ordered node. If the create operation fails, the client will start to retry the operation, and one step of the retry operation is to verify whether There exists a node containing this unique identifier.]:#

[A method is provided in the Curator client to guarantee the execution of the application's delete operation, and the Curator client will re-execute the operation until it succeeds, or when the Curator client instance is unavailable. To use this feature, it is only necessary to use the guaranteed method defined in the DeleteBuilder interface.]:#




<br>

### [back](./)
